{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89b6476-ff1d-41c3-8a7e-a0564652b031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Class mapping: {0: 'rock', 1: 'paper', 2: 'scissors', 3: 'unknown'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fg/04kgqyjj3cd0m17vb9llbf1h0000gn/T/ipykernel_17609/2623676613.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device)['model_state_dict'])\n",
      "I0000 00:00:1742003928.573944  819426 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 89.3), renderer: Apple M2 Pro\n",
      "W0000 00:00:1742003928.582478  820374 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1742003928.588452  820374 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define the CNN model structure (matching training)\n",
    "class RockPaperScissorsCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RockPaperScissorsCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 4)  # 4 output classes: rock, paper, scissors, unknown  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)  # Flatten the tensor\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load trained model\n",
    "model_path = \"rps_model_improved.pth\"\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "model = RockPaperScissorsCNN().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device)['model_state_dict'])\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# Define transformation for image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Must match training normalization\n",
    "])\n",
    "\n",
    "# Class mapping\n",
    "classes = ['rock', 'paper', 'scissors', 'unknown']\n",
    "print(f\"Class mapping: {dict(enumerate(classes))}\")\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Function to predict the class of a detected hand\n",
    "def predict_hand(image):\n",
    "    # Convert OpenCV image (NumPy array) to PIL image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image_pil = Image.fromarray(image)  # Convert NumPy array to PIL Image\n",
    "    image_pil = image_pil.resize((128, 128))  # Resize to match model input\n",
    "    \n",
    "    # Apply transformations\n",
    "    image_tensor = transform(image_pil).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Get model prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        probabilities = F.softmax(output, dim=1)[0]\n",
    "    \n",
    "    predicted_class = classes[predicted.item()]\n",
    "    \n",
    "    return predicted_class, {classes[i]: float(probabilities[i]) * 100 for i in range(len(classes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b19f7e2-a39b-4bab-a38d-a00e440c230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-14 18:58:50.423 python[17609:819426] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "W0000 00:00:1742003931.346879  820378 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "2025-03-14 18:58:51.574 python[17609:819426] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-03-14 18:58:51.574 python[17609:819426] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "# Start webcam capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert to RGB and process with MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get bounding box around the hand\n",
    "            h, w, _ = frame.shape\n",
    "            x_min = min([lm.x for lm in hand_landmarks.landmark]) * w\n",
    "            y_min = min([lm.y for lm in hand_landmarks.landmark]) * h\n",
    "            x_max = max([lm.x for lm in hand_landmarks.landmark]) * w\n",
    "            y_max = max([lm.y for lm in hand_landmarks.landmark]) * h\n",
    "            \n",
    "            x_min, y_min, x_max, y_max = int(x_min), int(y_min), int(x_max), int(y_max)\n",
    "\n",
    "            # Ensure bounding box stays within frame\n",
    "            x_min, y_min = max(0, x_min), max(0, y_min)\n",
    "            x_max, y_max = min(w, x_max), min(h, y_max)\n",
    "\n",
    "            # Extract and predict hand gesture\n",
    "            hand_crop = frame[y_min:y_max, x_min:x_max]\n",
    "            if hand_crop.size > 0:\n",
    "                predicted_class, probabilities = predict_hand(hand_crop)\n",
    "\n",
    "                # Draw bounding box & label\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                label = f\"{predicted_class} ({probabilities[predicted_class]:.2f}%)\"\n",
    "                cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "    \n",
    "    # Show live feed\n",
    "    cv2.imshow(\"Rock-Paper-Scissors Detection\", frame)\n",
    "    \n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5477728c-92ba-4586-85f5-fc3bf5439bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define CNN Model (Must match training)\n",
    "class RockPaperScissorsCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RockPaperScissorsCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, 4)  # 4 classes: rock, paper, scissors, unknown  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.shape[0], -1)  # Flatten\n",
    "        x = F.relu(self.fc1(x)) \n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Load trained model\n",
    "model_path = \"rps_model_improved.pth\"\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "model = RockPaperScissorsCNN().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device)['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Define transformation for input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "# Class labels\n",
    "classes = ['rock', 'paper', 'scissors', 'unknown']\n",
    "print(f\"Class mapping: {dict(enumerate(classes))}\")\n",
    "\n",
    "# Initialize MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Fixed Bounding Box Size\n",
    "BOX_SIZE = 400  # Fixed size in pixels\n",
    "\n",
    "# Function to classify hand gesture\n",
    "def predict_hand(image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "    image_pil = Image.fromarray(image).resize((128, 128))  # Convert to PIL and resize\n",
    "    \n",
    "    # Apply transformations\n",
    "    image_tensor = transform(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        probabilities = F.softmax(output, dim=1)[0]\n",
    "\n",
    "    predicted_class = classes[predicted.item()]\n",
    "    return predicted_class, {classes[i]: float(probabilities[i]) * 100 for i in range(len(classes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9040ee1b-ed04-4875-af44-8964b06eecb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    h, w, _ = frame.shape  # Get frame dimensions\n",
    "\n",
    "    # Convert to RGB and process with MediaPipe\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Get center of hand\n",
    "            x_center = int(np.mean([lm.x for lm in hand_landmarks.landmark]) * w)\n",
    "            y_center = int(np.mean([lm.y for lm in hand_landmarks.landmark]) * h)\n",
    "\n",
    "            # Define fixed bounding box around the detected hand\n",
    "            x_min = max(0, x_center - BOX_SIZE // 2)\n",
    "            y_min = max(0, y_center - BOX_SIZE // 2)\n",
    "            x_max = min(w, x_center + BOX_SIZE // 2)\n",
    "            y_max = min(h, y_center + BOX_SIZE // 2)\n",
    "\n",
    "            # Extract hand region inside fixed-size box\n",
    "            hand_crop = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "            # Ensure valid extraction\n",
    "            if hand_crop.shape[0] > 0 and hand_crop.shape[1] > 0:\n",
    "                predicted_class, probabilities = predict_hand(hand_crop)\n",
    "\n",
    "                # Display classification result\n",
    "                label = f\"{predicted_class} ({probabilities[predicted_class]:.2f}%)\"\n",
    "                cv2.putText(frame, label, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            # Draw the fixed-size bounding box\n",
    "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "\n",
    "            # Draw hand landmarks\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Rock-Paper-Scissors Detection\", frame)\n",
    "\n",
    "    # Press 'q' to exit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15359b59-7cf5-47e1-9a25-a5c11ef82654",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rps_detection)",
   "language": "python",
   "name": "rps_detection"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
